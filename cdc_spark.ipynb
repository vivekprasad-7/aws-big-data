{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql import types as T\nfrom datetime import datetime\nimport logging\ndf_target = spark.createDataFrame([(11,'Sam',1000,'ind','IT','2/11/2019'),(22,'Tom',2000,'usa','HR','2/11/2019'),\n                                 (33,'Kom',3500,'uk','IT','2/11/2019'),(44,'Nom',4000,'can','HR','2/11/2019'),\n                                 (55,'Vom',5000,'mex','IT','2/11/2019'),(66,'XYZ',5000,'mex','IT','2/11/2019')],\n                                 ['Id','Name','Sal','Address','Dept','Join_Date']) \ndf_source = spark.createDataFrame([(11,'Sam',1000,'ind','IT','2/11/2019'),(22,'Tom',2000,'usa','HR','2/11/2019'),\n                                  (33,'Kom',3000,'GB','MKT','12/12/2019'),(44,'Nom',4000,'can','HR','2/11/2019'),\n                                  (45,'Xom',5000,'mex','IT','2/11/2019'),(77,'XYZ',5000,'mex','IT','2/11/2019')],\n                                  ['Id','Name','Sal','Address','Dept','Join_Date']) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"07a7173c-fe75-4beb-a7e5-6bf440d8d213","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#target = add_metadata(df_target)\n#source = add_metadata(df_source)\ntarget = df_target\nsource = df_source\ncols = target.columns[1:]\nprint(cols)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4b47ceff-520b-4bc1-8418-aa468363a9a7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"['Name', 'Sal', 'Address', 'Dept', 'Join_Date']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['Name', 'Sal', 'Address', 'Dept', 'Join_Date']\n"]}}],"execution_count":0},{"cell_type":"code","source":["def check_changes(col1,col2):\n  compare_cols = (col1 == col2)\n  return ((col1.isNull()) & (col2.isNull())) | compare_cols\n\ndef add_to_string_col(col,fail_string,check_col):\n \"\"\"\n append a string to error_col\n \"\"\"\n fail_text = (F.when((check_col == True), \"\")\n              .otherwise(F.when(col == \"\", fail_string)\n                         .otherwise(f\",{fail_string}\")))\n return F.concat(col, fail_text)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9a670f7-348f-4071-8c42-27c13e8a4e4b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def find_changes(logging,df_source_exists,cols):\n  change_col = \"data_changes\"\n  col = df_source_exists.columns[0]\n  logging.info('Using ID column: ' + col)\n  print(df_source_exists.columns)\n  df_changes = df_source_exists.withColumn(change_col,F.lit(\"\"))\n \n  logging.info('Tracking changes for:')\n  loop_count = 0\n  for col in cols:\n    logging.info(col)\n    print(col)\n    loop_count += 1\n    df_changes = df_changes.withColumn(change_col, add_to_string_col(\n      df_changes[change_col],f\"{col}\",\n      check_changes(df_changes[col],df_changes[col+'_t'])))\n    # incase if you want to avoid over writing previous non null values\n    # df_changes = df_changes.withColumn(col,F.coalesce(df_changes[col],df_changes[col+'_t']))\n    if loop_count %3 == 0:\n      df_changes.cache().count()\n      \n  return df_changes\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6bbf232e-2625-4a65-bf35-8ec0c67787f5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def process_save_changes(logging,df_source,df_target,change_cols):\n  \"\"\"\n  identify changes in two dataframes\n  \n  parameters:\n    df_source - dataframe containing new dataset\n    df_target - existing datasets (target dataframe will updated accordingly)\n    change_cols - list of varibles to for capturing chnages    \n  \"\"\"\n  logging.info('START - identify changes: {}'.format(datetime.now().strftime(\"%Y%m%d%H%M\")))\n  print('START - identify changes: {}'.format(datetime.now().strftime(\"%Y%m%d%H%M\")))\n  change_col = \"data_changes\"\n  col = df_source.columns[0]\n  \n  # separate target data by live_flag\n  df_active = df_target#.filter(df_target['live_flag'] == True) \n  df_inactive = df_target#.filter(df_target['live_flag'] == False) \n  df_active.cache().count()\n  df_inactive.cache().count()\n  df_source.cache().count()\n\n  # records already exist\n  df_t = df_active.select(*(F.col(x).alias(x + '_t') for x in df_active.columns))\n  df_amends = df_source.join(df_t,df_source[col] == df_t[col+'_t'],'inner')\\\n                           .select('*')\\\n                           .dropDuplicates([col]) \n  \n  # birth\n  df_birth = df_source.join(df_target, on=[col], how='leftanti')\n  df_birth = df_birth.withColumn(change_col,F.lit(2))\n  #df_birth.show(truncate=False)   \n  \n  df_birth.cache().count()\n  \n  # death\n  df_death = df_target.join(df_source, on=[col], how='leftanti')\n  df_death = df_death.withColumn(change_col,F.lit(99))\n  #df_death.show(truncate=False)   \n  df_death.cache().count()\n  \n  # identify updated records\n  df_changes = find_changes(logging,df_amends,change_cols)\n  df_changes = df_changes.drop(*df_t.columns)\n  #df_changes.show(truncate=False)   \n  \n  # join new and updated source records\n  df_all_changes = df_birth.union(df_changes)\n  df_all_changes = df_all_changes.union(df_death)\n  #df_all_changes.show(truncate=False)   \n                             \n  return  df_all_changes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e8f35572-18e4-4b3c-86dd-191d6c4397cb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\ndf = process_save_changes(logging,source,target,cols)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1d615fab-95e1-4bce-9379-34e20645c263","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"START - identify changes: 202302271311\n['Id', 'Name', 'Sal', 'Address', 'Dept', 'Join_Date', 'Id_t', 'Name_t', 'Sal_t', 'Address_t', 'Dept_t', 'Join_Date_t']\nName\nSal\nAddress\nDept\nJoin_Date\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["START - identify changes: 202302271311\n['Id', 'Name', 'Sal', 'Address', 'Dept', 'Join_Date', 'Id_t', 'Name_t', 'Sal_t', 'Address_t', 'Dept_t', 'Join_Date_t']\nName\nSal\nAddress\nDept\nJoin_Date\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c242c704-8fa2-4ce8-9d16-b49073ff387b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---+----+----+-------+----+----------+--------------------------+\n|Id |Name|Sal |Address|Dept|Join_Date |data_changes              |\n+---+----+----+-------+----+----------+--------------------------+\n|45 |Xom |5000|mex    |IT  |2/11/2019 |2                         |\n|77 |XYZ |5000|mex    |IT  |2/11/2019 |2                         |\n|22 |Tom |2000|usa    |HR  |2/11/2019 |                          |\n|33 |Kom |3000|GB     |MKT |12/12/2019|Sal,Address,Dept,Join_Date|\n|44 |Nom |4000|can    |HR  |2/11/2019 |                          |\n|11 |Sam |1000|ind    |IT  |2/11/2019 |                          |\n|55 |Vom |5000|mex    |IT  |2/11/2019 |99                        |\n|66 |XYZ |5000|mex    |IT  |2/11/2019 |99                        |\n+---+----+----+-------+----+----------+--------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----+----+-------+----+----------+--------------------------+\n|Id |Name|Sal |Address|Dept|Join_Date |data_changes              |\n+---+----+----+-------+----+----------+--------------------------+\n|45 |Xom |5000|mex    |IT  |2/11/2019 |2                         |\n|77 |XYZ |5000|mex    |IT  |2/11/2019 |2                         |\n|22 |Tom |2000|usa    |HR  |2/11/2019 |                          |\n|33 |Kom |3000|GB     |MKT |12/12/2019|Sal,Address,Dept,Join_Date|\n|44 |Nom |4000|can    |HR  |2/11/2019 |                          |\n|11 |Sam |1000|ind    |IT  |2/11/2019 |                          |\n|55 |Vom |5000|mex    |IT  |2/11/2019 |99                        |\n|66 |XYZ |5000|mex    |IT  |2/11/2019 |99                        |\n+---+----+----+-------+----+----------+--------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"32058ada-4219-499f-b732-a31ee43c332a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"cdc_spark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1059933976750387}},"nbformat":4,"nbformat_minor":0}
